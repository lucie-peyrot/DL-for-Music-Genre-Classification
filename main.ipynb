{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2987387a",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d820d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f5908",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "## Loading the genre label of each track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cf06518c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m track_genre_df[\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m] = track_genre_df[\u001b[33m\"\u001b[39m\u001b[33mtrack_id\u001b[39m\u001b[33m\"\u001b[39m].apply(get_audio_path)\n\u001b[32m     28\u001b[39m track_genre_df = track_genre_df[track_genre_df[\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m].apply(os.path.isfile)]\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m track_genre_df[\u001b[33m\"\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mtrack_genre_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpath\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_loadable_audio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m track_genre_df = track_genre_df[track_genre_df[\u001b[33m\"\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m\"\u001b[39m]].reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     33\u001b[39m music_genres = track_genre_df[\u001b[33m\"\u001b[39m\u001b[33mmusic_genre\u001b[39m\u001b[33m\"\u001b[39m].unique().tolist()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4943\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4810\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4815\u001b[39m     **kwargs,\n\u001b[32m   4816\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4817\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4818\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4819\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4934\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4935\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4936\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4937\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4938\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4939\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4940\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4941\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4943\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mis_loadable_audio\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mis_loadable_audio\u001b[39m(path):\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         \u001b[43mlibrosa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msr\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmono\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:176\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    174\u001b[39m     \u001b[38;5;66;03m# Otherwise try soundfile first, and then fall back if necessary\u001b[39;00m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m         y, sr_native = \u001b[43m__soundfile_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m sf.SoundFileRuntimeError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    179\u001b[39m         \u001b[38;5;66;03m# If soundfile failed, try audioread instead\u001b[39;00m\n\u001b[32m    180\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, (\u001b[38;5;28mstr\u001b[39m, pathlib.PurePath)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:222\u001b[39m, in \u001b[36m__soundfile_load\u001b[39m\u001b[34m(path, offset, duration, dtype)\u001b[39m\n\u001b[32m    219\u001b[39m         frame_duration = -\u001b[32m1\u001b[39m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# Load the target number of frames, and transpose to match librosa form\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     y = \u001b[43msf_desc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_duration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m.T\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y, sr_native\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\soundfile.py:942\u001b[39m, in \u001b[36mSoundFile.read\u001b[39m\u001b[34m(self, frames, dtype, always_2d, fill_value, out)\u001b[39m\n\u001b[32m    940\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m frames < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m frames > \u001b[38;5;28mlen\u001b[39m(out):\n\u001b[32m    941\u001b[39m         frames = \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m frames = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_array_io\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mread\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) > frames:\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fill_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\soundfile.py:1394\u001b[39m, in \u001b[36mSoundFile._array_io\u001b[39m\u001b[34m(self, action, array, frames)\u001b[39m\n\u001b[32m   1392\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m array.dtype.itemsize == _ffi.sizeof(ctype)\n\u001b[32m   1393\u001b[39m cdata = _ffi.cast(ctype + \u001b[33m'\u001b[39m\u001b[33m*\u001b[39m\u001b[33m'\u001b[39m, array.__array_interface__[\u001b[33m'\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m'\u001b[39m][\u001b[32m0\u001b[39m])\n\u001b[32m-> \u001b[39m\u001b[32m1394\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cdata_io\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\soundfile.py:1403\u001b[39m, in \u001b[36mSoundFile._cdata_io\u001b[39m\u001b[34m(self, action, data, ctype, frames)\u001b[39m\n\u001b[32m   1401\u001b[39m     curr = \u001b[38;5;28mself\u001b[39m.tell()\n\u001b[32m   1402\u001b[39m func = \u001b[38;5;28mgetattr\u001b[39m(_snd, \u001b[33m'\u001b[39m\u001b[33msf_\u001b[39m\u001b[33m'\u001b[39m + action + \u001b[33m'\u001b[39m\u001b[33mf_\u001b[39m\u001b[33m'\u001b[39m + ctype)\n\u001b[32m-> \u001b[39m\u001b[32m1403\u001b[39m frames = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1404\u001b[39m _error_check(\u001b[38;5;28mself\u001b[39m._errorcode)\n\u001b[32m   1405\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.seekable():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Loading the information about each track (metadata)\n",
    "path_of_fma_metadata = \"../fma_metadata/tracks.csv\"\n",
    "fma_metadata = pd.read_csv(path_of_fma_metadata, header=[0, 1], index_col=0)\n",
    "# Flatten columns multi-index\n",
    "fma_metadata.columns = ['__'.join(col).strip() for col in fma_metadata.columns.values]\n",
    "\n",
    "# Keeping only the genre column\n",
    "track_genre_df = fma_metadata.reset_index()[['track_id', 'track__genre_top']]\n",
    "track_genre_df = track_genre_df.rename(columns={\"track__genre_top\": \"music_genre\"})\n",
    "track_genre_df = track_genre_df.dropna(subset=[\"music_genre\"])\n",
    "\n",
    "\n",
    "# Function to get the audio file path from track_id\n",
    "def get_audio_path(track_id):\n",
    "    # There are 1000 tracks per folder\n",
    "    folder = str(track_id // 1000).zfill(3)\n",
    "    filename = f\"{str(track_id).zfill(6)}.mp3\"\n",
    "    return \"../fma_small\" + f\"/{folder}/{filename}\"\n",
    "\n",
    "def is_loadable_audio(path):\n",
    "    try:\n",
    "        librosa.load(path, sr=None, mono=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "track_genre_df[\"path\"] = track_genre_df[\"track_id\"].apply(get_audio_path)\n",
    "track_genre_df = track_genre_df[track_genre_df[\"path\"].apply(os.path.isfile)]\n",
    "track_genre_df[\"valid\"] = track_genre_df[\"path\"].apply(is_loadable_audio)\n",
    "track_genre_df = track_genre_df[track_genre_df[\"valid\"]].reset_index(drop=True)\n",
    "\n",
    "\n",
    "music_genres = track_genre_df[\"music_genre\"].unique().tolist()\n",
    "nbr_music_genres = len(music_genres)\n",
    "genre_to_idx = {g: i for i, g in enumerate(music_genres)}\n",
    "track_genre_df[\"label\"] = track_genre_df[\"music_genre\"].map(genre_to_idx)\n",
    "print(track_genre_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad9991b",
   "metadata": {},
   "source": [
    "## Preprocessing the data with Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3221788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precomputing mel-spectrogram and puting in cache\n",
    "CACHE_DIR = \"./mel_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "MAX_LEN = 1300\n",
    "\n",
    "def compute_mel(path):\n",
    "    audio, _ = librosa.load(path, sr=None, mono=True)\n",
    "    mel = librosa.feature.melspectrogram(y=audio, n_mels=128)\n",
    "    mel = librosa.power_to_db(mel).astype(np.float32)\n",
    "\n",
    "    # Normalize\n",
    "    mel = (mel - mel.mean()) / (mel.std() + 1e-6)\n",
    "    T = mel.shape[1]\n",
    "    if T < MAX_LEN:\n",
    "        mel = np.pad(mel, ((0,0),(0,MAX_LEN-T)))\n",
    "    else:\n",
    "        mel = mel[:, :MAX_LEN]\n",
    "\n",
    "    return mel\n",
    "\n",
    "print(\"Precomputing mel-spectrogram and puting in cache\")\n",
    "cached_paths = []\n",
    "for i, row in tqdm(track_genre_df.iterrows(), total=len(track_genre_df)):\n",
    "    track_id = row[\"track_id\"]\n",
    "    audio_path = row[\"path\"]\n",
    "\n",
    "    cache_path = f\"{CACHE_DIR}/{track_id}.npy\"\n",
    "    cached_paths.append(cache_path)\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        mel = compute_mel(audio_path)\n",
    "        np.save(cache_path, mel)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "\n",
    "track_genre_df[\"mel_path\"] = cached_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenreDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.genre_to_idx = {g:i for i,g in enumerate(df[\"music_genre\"].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        mel = np.load(row[\"mel_path\"])\n",
    "        mel = torch.tensor(mel, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        label = self.genre_to_idx[row[\"music_genre\"]]\n",
    "        return mel, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390354ee",
   "metadata": {},
   "source": [
    "## Spliting the Dataset into Train/Test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ab196",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = track_genre_df.index[\n",
    "    track_genre_df[\"path\"].apply(lambda p: os.path.isfile(p))\n",
    "].tolist()\n",
    "\n",
    "\n",
    "rd.shuffle(indices)\n",
    "\n",
    "split = int(0.8 * len(indices))\n",
    "train_indices = indices[:split]\n",
    "test_indices = indices[split:]\n",
    "\n",
    "train_dataset = Subset(MusicGenreDataset(track_genre_df), train_indices)\n",
    "test_dataset = Subset(MusicGenreDataset(track_genre_df), test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423471db",
   "metadata": {},
   "source": [
    "# Baseline Implementation\n",
    "## Model : \n",
    "Small (2D) CNN with 3 convolutional layers (Conv → ReLU → MaxPool →\n",
    "Dropout), followed by a fully connected layer and softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our baseline model\n",
    "class CNN_base(nn.Module):\n",
    "    def __init__(self, num_classes=nbr_music_genres, dropout=0.25):\n",
    "        super().__init__()\n",
    "        channels = [1, 32, 64, 128]\n",
    "        layers = []\n",
    "        for i in range(3):\n",
    "            layers += [\n",
    "                nn.Conv2d(channels[i], channels[i + 1], kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout2d(dropout)\n",
    "            ]\n",
    "           \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)   \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47289d9",
   "metadata": {},
   "source": [
    " • Loss: Cross-entropy.\n",
    "\n",
    " • Optimizer: Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94424233",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_base(num_classes=nbr_music_genres, dropout=0.25)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851f5f8",
   "metadata": {},
   "source": [
    " • Metric: Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dl, device, writer=None, global_step=None):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for mel,label in test_dl:\n",
    "            mel = mel.to(device)\n",
    "            label = label.to(device)\n",
    "            preds = model(mel).argmax(1)\n",
    "            correct += (preds == label).sum().item()\n",
    "            total += label.size(0)\n",
    "    if writer is not None and global_step is not None:\n",
    "        writer.add_scalar(\"test/accuracy\", 100 * correct / total, global_step)\n",
    "    print(f\"Test accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3fd5e2",
   "metadata": {},
   "source": [
    "# Run the training loop\n",
    "\n",
    "Use \n",
    "[tensorboard]\n",
    "( https://docs.pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) \n",
    "to monitor training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382a2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "\n",
    "# log a small batch of images and the model graph (if possible)\n",
    "imgs_sample, labels_sample = next(iter(train_dl))\n",
    "imgs_sample = imgs_sample.to(device)\n",
    "grid = make_grid(imgs_sample[:16], nrow=4, normalize=True, scale_each=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313417cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucie\\AppData\\Local\\Temp\\ipykernel_12972\\2829234150.py:13: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, _ = librosa.load(path, sr=None, mono=True)\n",
      "c:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 11.81%\n",
      "Epoch 1: train loss = 2.0889\n",
      "Test accuracy: 15.69%\n",
      "Epoch 2: train loss = 2.0334\n",
      "Test accuracy: 20.69%\n",
      "Epoch 3: train loss = 1.8320\n",
      "Test accuracy: 25.88%\n"
     ]
    }
   ],
   "source": [
    "# provide a global step counter that you can increment in the training loop if desired\n",
    "global_step = 0\n",
    "\n",
    "test_model(model, test_dl, device, global_step=global_step)\n",
    "for epoch in range(3):\n",
    "    for mel, labels in train_dl:\n",
    "        mel, labels = mel.to(device), labels.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        logits = model(mel)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")\n",
    "    test_model(model, test_dl, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
