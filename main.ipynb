{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2987387a",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "66d820d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import librosa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6e5ff",
   "metadata": {},
   "source": [
    "## Reproductibility : adding training seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bcb5da64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x26221e89c70>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 0\n",
    "rd.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f5908",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "## Loading the genre label of each track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cf06518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      track_id music_genre                         path  exists  label\n",
      "0            2     Hip-Hop  ..\\fma_small\\000\\000002.mp3    True      0\n",
      "1            5     Hip-Hop  ..\\fma_small\\000\\000005.mp3    True      0\n",
      "2           10         Pop  ..\\fma_small\\000\\000010.mp3    True      1\n",
      "3          140        Folk  ..\\fma_small\\000\\000140.mp3    True      2\n",
      "4          141        Folk  ..\\fma_small\\000\\000141.mp3    True      2\n",
      "...        ...         ...                          ...     ...    ...\n",
      "7995    154308     Hip-Hop  ..\\fma_small\\154\\154308.mp3    True      0\n",
      "7996    154309     Hip-Hop  ..\\fma_small\\154\\154309.mp3    True      0\n",
      "7997    154413         Pop  ..\\fma_small\\154\\154413.mp3    True      1\n",
      "7998    154414         Pop  ..\\fma_small\\154\\154414.mp3    True      1\n",
      "7999    155066     Hip-Hop  ..\\fma_small\\155\\155066.mp3    True      0\n",
      "\n",
      "[8000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Loading the information about each track (metadata)\n",
    "path_of_fma_metadata = \"../fma_metadata/tracks.csv\"\n",
    "fma_metadata = pd.read_csv(path_of_fma_metadata, header=[0, 1], index_col=0)\n",
    "# Flatten columns multi-index\n",
    "fma_metadata.columns = ['__'.join(col).strip() for col in fma_metadata.columns.values]\n",
    "\n",
    "# Keeping only the genre column\n",
    "track_genre_df = fma_metadata.reset_index()[['track_id', 'track__genre_top']]\n",
    "track_genre_df = track_genre_df.rename(columns={\"track__genre_top\": \"music_genre\"})\n",
    "track_genre_df = track_genre_df.dropna(subset=[\"music_genre\"])\n",
    "\n",
    "\n",
    "# Function to get the audio file path from track_id\n",
    "def make_audio_path(track_id):\n",
    "    folder = f\"{track_id // 1000:03d}\"\n",
    "    filename = f\"{track_id:06d}.mp3\"\n",
    "    return Path(\"../fma_small\") / folder / filename\n",
    "\n",
    "track_genre_df[\"path\"] = track_genre_df[\"track_id\"].apply(make_audio_path)\n",
    "\n",
    "all_valid_files = {\n",
    "    p.resolve() for p in Path(\"../fma_small\").rglob(\"*.mp3\")\n",
    "}\n",
    "\n",
    "# Keep only rows where path exists\n",
    "track_genre_df[\"exists\"] = track_genre_df[\"path\"].apply(lambda p: p.resolve() in all_valid_files)\n",
    "track_genre_df = track_genre_df[track_genre_df[\"exists\"]].reset_index(drop=True)\n",
    "\n",
    "\n",
    "music_genres = track_genre_df[\"music_genre\"].unique().tolist()\n",
    "nbr_music_genres = len(music_genres)\n",
    "genre_to_idx = {g: i for i, g in enumerate(music_genres)}\n",
    "track_genre_df[\"label\"] = track_genre_df[\"music_genre\"].map(genre_to_idx)\n",
    "print(track_genre_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad9991b",
   "metadata": {},
   "source": [
    "## Preprocessing the data with Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3221788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucie\\AppData\\Local\\Temp\\ipykernel_24804\\4120235938.py:13: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, _ = librosa.load(path, sr=None, mono=True)\n",
      "c:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    }
   ],
   "source": [
    "#Precomputing mel-spectrogram and puting in cache\n",
    "CACHE_DIR = \"./mel_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "MAX_LEN = 2600 # To unifirm the length of the mel-spectrograms\n",
    " \n",
    "def compute_and_cache_mel(track_id, path):\n",
    "    cache_path = f\"{CACHE_DIR}/{track_id}.npy\"\n",
    "\n",
    "    if os.path.isfile(cache_path):\n",
    "        return cache_path  # already cached\n",
    "\n",
    "    try:\n",
    "        audio, _ = librosa.load(path, sr=None, mono=True)\n",
    "        mel = librosa.feature.melspectrogram(y=audio, n_mels=128)\n",
    "        mel_db = librosa.power_to_db(mel)\n",
    "\n",
    "        # normalize\n",
    "        mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-6)\n",
    "\n",
    "        T = mel_db.shape[1]\n",
    "        if T < MAX_LEN:\n",
    "            mel_db = np.pad(mel_db, ((0, 0), (0, MAX_LEN - T)), mode=\"constant\")\n",
    "        else:\n",
    "            mel_db = mel_db[:, :MAX_LEN]\n",
    "\n",
    "        \n",
    "        np.save(cache_path, mel_db.astype(np.float32))\n",
    "        return cache_path\n",
    "\n",
    "    except Exception:\n",
    "        return None \n",
    "\n",
    "mel_paths = []\n",
    "\n",
    "for i, row in track_genre_df.iterrows():\n",
    "    cache_path = compute_and_cache_mel(row[\"track_id\"], row[\"path\"])\n",
    "    mel_paths.append(cache_path)\n",
    "\n",
    "track_genre_df[\"mel_path\"] = mel_paths\n",
    "\n",
    "# keep only rows with valid cached mels\n",
    "track_genre_df = track_genre_df[track_genre_df[\"mel_path\"].notna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1937f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenreDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.genre_to_idx = {g:i for i,g in enumerate(df[\"music_genre\"].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        mel = np.load(row[\"mel_path\"])\n",
    "        mel = torch.tensor(mel, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        label = self.genre_to_idx[row[\"music_genre\"]]\n",
    "        return mel, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390354ee",
   "metadata": {},
   "source": [
    "## Spliting the Dataset into Train/Test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9c1ab196",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = track_genre_df.index[\n",
    "    track_genre_df[\"path\"].apply(lambda p: os.path.isfile(p))\n",
    "].tolist()\n",
    "\n",
    "\n",
    "rd.shuffle(indices)\n",
    "\n",
    "split = int(0.8 * len(indices))\n",
    "train_indices = indices[:split]\n",
    "test_indices = indices[split:]\n",
    "\n",
    "train_dataset = Subset(MusicGenreDataset(track_genre_df), train_indices)\n",
    "test_dataset = Subset(MusicGenreDataset(track_genre_df), test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423471db",
   "metadata": {},
   "source": [
    "# Baseline Implementation\n",
    "## Model : \n",
    "Small (2D) CNN with 4 convolutional layers (Conv → ReLU → MaxPool →\n",
    "Dropout), followed by a fully connected layer and softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "593b129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our baseline model\n",
    "class CNN_base(nn.Module):\n",
    "    def __init__(self, num_classes=nbr_music_genres, dropout=0.25):\n",
    "        super().__init__()\n",
    "        channels = [1, 32, 64, 128, 256]\n",
    "        layers = []\n",
    "        for i in range(4):\n",
    "            layers += [\n",
    "                nn.Conv2d(channels[i], channels[i + 1], kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(channels[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout2d(dropout)\n",
    "            ]\n",
    "           \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)   \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47289d9",
   "metadata": {},
   "source": [
    " • Loss: Cross-entropy.\n",
    "\n",
    " • Optimizer: Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "94424233",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_base(num_classes=nbr_music_genres, dropout=0.25)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851f5f8",
   "metadata": {},
   "source": [
    " • Metric: Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8cd01df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_confusion_matrix(labels, preds, num_classes):\n",
    "    with torch.no_grad():\n",
    "        k = (labels >= 0) & (labels < num_classes)\n",
    "        inds = num_classes * labels[k] + preds[k]\n",
    "        cm = torch.bincount(inds, minlength=num_classes**2).reshape(num_classes, num_classes)\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dl, device, writer=None, global_step=None, class_names=None,name=\"\"):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for mel,label in test_dl:\n",
    "            mel = mel.to(device)\n",
    "            label = label.to(device)\n",
    "            preds = model(mel)\n",
    "            correct += (preds.argmax(1) == label).sum().item()\n",
    "            total += label.size(0)\n",
    "            losses.append(criterion(preds, label).mean().numpy())\n",
    "            all_preds.append(preds.argmax(1).cpu())\n",
    "            all_labels.append(label.cpu())\n",
    "    if writer is not None and global_step is not None:\n",
    "        writer.add_scalar(f\"{name}/test/accuracy\", 100 * correct / total, global_step)\n",
    "        writer.add_scalar(f\"{name}/test/loss\", np.mean(losses), global_step)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "\n",
    "        num_classes = len(class_names) if class_names else 8\n",
    "        cm = torch.zeros((8, 8), dtype=torch.int64)\n",
    "        for t, p in zip(all_labels, all_preds):\n",
    "            cm[t, p] += 1\n",
    "\n",
    "        # TensorBoard display\n",
    "        \n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=False, cmap=\"Blues\", \n",
    "                    xticklabels=class_names if class_names else range(num_classes),\n",
    "                    yticklabels=class_names if class_names else range(num_classes))\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "\n",
    "        writer.add_figure(\"test/confusion_matrix\", fig, global_step)\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f\"Test accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3fd5e2",
   "metadata": {},
   "source": [
    "# Run the training loop\n",
    "\n",
    "Use \n",
    "[tensorboard]\n",
    "( https://docs.pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) \n",
    "to monitor training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "382a2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "# log a small batch of images and the model graph (if possible)\n",
    "imgs_sample, labels_sample = next(iter(train_dl))\n",
    "imgs_sample = imgs_sample.to(device)\n",
    "grid = make_grid(imgs_sample[:16], nrow=4, normalize=True, scale_each=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "313417cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected floating point type for target with class probabilities, got Long",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[115]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m global_step = \u001b[32m0\u001b[39m\n\u001b[32m      3\u001b[39m writer = SummaryWriter(log_dir=\u001b[33m\"\u001b[39m\u001b[33m./runs/genre_classif\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtest_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglobal_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m3\u001b[39m):\n\u001b[32m      7\u001b[39m     model.train()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[113]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtest_model\u001b[39m\u001b[34m(model, test_dl, device, writer, global_step, class_names, name)\u001b[39m\n\u001b[32m     12\u001b[39m correct += (preds == label).sum().item()\n\u001b[32m     13\u001b[39m total += label.size(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m losses.append(\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m.mean().numpy())\n\u001b[32m     15\u001b[39m all_preds.append(preds.cpu())\n\u001b[32m     16\u001b[39m all_labels.append(label.cpu())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1385\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m   1384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1385\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1386\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1387\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1389\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1390\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1391\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\functional.py:3458\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3456\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3457\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3458\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3459\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3460\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3461\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3462\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3463\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3464\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3465\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected floating point type for target with class probabilities, got Long"
     ]
    }
   ],
   "source": [
    "# provide a global step counter that you can increment in the training loop if desired\n",
    "global_step = 0\n",
    "writer = SummaryWriter(log_dir=\"./runs/genre_classif\")\n",
    "\n",
    "test_model(model, test_dl, device, global_step=global_step)\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    train_bar = tqdm(train_dl, desc=f\"Epoch {epoch+1}\")\n",
    "    for mel, labels in train_dl:\n",
    "        global_step += 1\n",
    "        mel, labels = mel.to(device), labels.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        logits = model(mel)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        writer.add_scalar(\"train/cnn_loss\", loss.item(), global_step)\n",
    "        train_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    test_model(model, test_dl, device, writer, global_step, class_names=music_genres)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
