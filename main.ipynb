{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2987387a",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66d820d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import os\n",
    "\n",
    "import librosa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f5908",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "## Loading the genre label of each track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf06518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      track_id music_genre                         path  label\n",
      "0            2     Hip-Hop  ../fma_small/000/000002.mp3      0\n",
      "1            5     Hip-Hop  ../fma_small/000/000005.mp3      0\n",
      "2           10         Pop  ../fma_small/000/000010.mp3      1\n",
      "3          140        Folk  ../fma_small/000/000140.mp3      2\n",
      "4          141        Folk  ../fma_small/000/000141.mp3      2\n",
      "...        ...         ...                          ...    ...\n",
      "7995    154308     Hip-Hop  ../fma_small/154/154308.mp3      0\n",
      "7996    154309     Hip-Hop  ../fma_small/154/154309.mp3      0\n",
      "7997    154413         Pop  ../fma_small/154/154413.mp3      1\n",
      "7998    154414         Pop  ../fma_small/154/154414.mp3      1\n",
      "7999    155066     Hip-Hop  ../fma_small/155/155066.mp3      0\n",
      "\n",
      "[8000 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Loading the information about each track (metadata)\n",
    "path_of_fma_metadata = \"../fma_metadata/tracks.csv\"\n",
    "fma_metadata = pd.read_csv(path_of_fma_metadata, header=[0, 1], index_col=0)\n",
    "# Flatten columns multi-index\n",
    "fma_metadata.columns = ['__'.join(col).strip() for col in fma_metadata.columns.values]\n",
    "\n",
    "# Keeping only the genre column\n",
    "track_genre_df = fma_metadata.reset_index()[['track_id', 'track__genre_top']]\n",
    "track_genre_df = track_genre_df.rename(columns={\"track__genre_top\": \"music_genre\"})\n",
    "track_genre_df = track_genre_df.dropna(subset=[\"music_genre\"])\n",
    "\n",
    "\n",
    "# Function to get the audio file path from track_id\n",
    "def get_audio_path(track_id):\n",
    "    # There are 1000 tracks per folder\n",
    "    folder = str(track_id // 1000).zfill(3)\n",
    "    filename = f\"{str(track_id).zfill(6)}.mp3\"\n",
    "    return \"../fma_small\" + f\"/{folder}/{filename}\"\n",
    "\n",
    "track_genre_df[\"path\"] = track_genre_df[\"track_id\"].apply(get_audio_path)\n",
    "track_genre_df = track_genre_df[track_genre_df[\"path\"].apply(os.path.isfile)]\n",
    "track_genre_df = track_genre_df.reset_index(drop=True) \n",
    "music_genres = track_genre_df[\"music_genre\"].unique().tolist()\n",
    "nbr_music_genres = len(music_genres)\n",
    "genre_to_idx = {g: i for i, g in enumerate(music_genres)}\n",
    "track_genre_df[\"label\"] = track_genre_df[\"music_genre\"].map(genre_to_idx)\n",
    "print(track_genre_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad9991b",
   "metadata": {},
   "source": [
    "## Preprocessing the data with Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1937f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenreDataset(torch.utils.data.Dataset):\n",
    "    MAX_LEN = 1300   \n",
    "    def __init__(self, track_genre_df, transform=None):\n",
    "        self.track_genre_df = track_genre_df\n",
    "        self.transform = transform\n",
    "        self.genre_to_idx = {g:i for i,g in enumerate(track_genre_df[\"music_genre\"].unique())}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.track_genre_df)\n",
    "\n",
    "    def load_audio(self, path):\n",
    "        try:\n",
    "            audio, _ = librosa.load(path, sr=None, mono=True)\n",
    "            return audio\n",
    "        except:\n",
    "            return np.zeros(22050, dtype=np.float32)\n",
    "\n",
    "    \n",
    "    def audio_to_mel(self, audio):\n",
    "        mel = librosa.feature.melspectrogram(y=audio, n_mels=128)\n",
    "        mel_db = librosa.power_to_db(mel)\n",
    "        return mel_db.astype(np.float32)\n",
    "\n",
    "    def track_normalization(self, mel):\n",
    "        mean = mel.mean()\n",
    "        std = mel.std() if mel.std() > 1e-6 else 1.0\n",
    "        mel_normalized = (mel - mean) / std\n",
    "        T = mel_normalized.shape[1]\n",
    "        if T < self.MAX_LEN:\n",
    "            mel_croped = np.pad(mel_normalized, ((0, 0), (0, self.MAX_LEN - T)), mode=\"constant\")\n",
    "        else:\n",
    "            mel_croped = mel_normalized[:, :self.MAX_LEN]\n",
    "        return mel_croped\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.track_genre_df.iloc[idx]\n",
    "        audio = self.load_audio(row[\"path\"])\n",
    "        mel = self.audio_to_mel(audio)\n",
    "        mel = self.track_normalization(mel)\n",
    "\n",
    "        # shape -> (1, 128, time)\n",
    "        mel = np.asarray(mel, dtype=np.float32)           \n",
    "        mel = np.nan_to_num(mel) \n",
    "        mel = torch.tensor(mel, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "\n",
    "        label = self.genre_to_idx[row[\"music_genre\"]]\n",
    "        return mel, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390354ee",
   "metadata": {},
   "source": [
    "## Spliting the Dataset into Train/Test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c1ab196",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = track_genre_df.index[\n",
    "    track_genre_df[\"path\"].apply(lambda p: os.path.isfile(p))\n",
    "].tolist()\n",
    "\n",
    "\n",
    "rd.shuffle(indices)\n",
    "\n",
    "split = int(0.8 * len(indices))\n",
    "train_indices = indices[:split]\n",
    "test_indices = indices[split:]\n",
    "\n",
    "train_dataset = Subset(MusicGenreDataset(track_genre_df), train_indices)\n",
    "test_dataset = Subset(MusicGenreDataset(track_genre_df), test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423471db",
   "metadata": {},
   "source": [
    "# Baseline Implementation\n",
    "## Model : \n",
    "Small (2D) CNN with 3 convolutional layers (Conv → ReLU → MaxPool →\n",
    "Dropout), followed by a fully connected layer and softmax output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "593b129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our baseline model\n",
    "class CNN_base(nn.Module):\n",
    "    def __init__(self, num_classes=nbr_music_genres, dropout=0.25):\n",
    "        super().__init__()\n",
    "        channels = [1, 32, 64, 128]\n",
    "        layers = []\n",
    "        for i in range(3):\n",
    "            layers += [\n",
    "                nn.Conv2d(channels[i], channels[i + 1], kernel_size=3, padding=1),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout2d(dropout)\n",
    "            ]\n",
    "           \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes),\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)   \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47289d9",
   "metadata": {},
   "source": [
    " • Loss: Cross-entropy.\n",
    "\n",
    " • Optimizer: Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94424233",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_base(num_classes=nbr_music_genres, dropout=0.25)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851f5f8",
   "metadata": {},
   "source": [
    " • Metric: Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8218197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dl, device, writer=None, global_step=None):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for mel,label in test_dl:\n",
    "            mel = mel.to(device)\n",
    "            label = label.to(device)\n",
    "            preds = model(mel).argmax(1)\n",
    "            correct += (preds == label).sum().item()\n",
    "            total += label.size(0)\n",
    "    if writer is not None and global_step is not None:\n",
    "        writer.add_scalar(\"test/accuracy\", 100 * correct / total, global_step)\n",
    "    print(f\"Test accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3fd5e2",
   "metadata": {},
   "source": [
    "# Run the training loop\n",
    "\n",
    "Use \n",
    "[tensorboard]\n",
    "( https://docs.pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) \n",
    "to monitor training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "382a2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=4)\n",
    "\n",
    "\n",
    "# log a small batch of images and the model graph (if possible)\n",
    "imgs_sample, labels_sample = next(iter(train_dl))\n",
    "imgs_sample = imgs_sample.to(device)\n",
    "grid = make_grid(imgs_sample[:16], nrow=4, normalize=True, scale_each=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "313417cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucie\\AppData\\Local\\Temp\\ipykernel_12972\\3296110197.py:13: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, _ = librosa.load(path, sr=None, mono=True)\n",
      "c:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 14.44%\n",
      "Epoch 1: train loss = 1.9360\n",
      "Test accuracy: 14.25%\n",
      "Epoch 2: train loss = 2.0027\n",
      "Test accuracy: 21.25%\n",
      "Epoch 3: train loss = 1.7514\n",
      "Test accuracy: 25.00%\n"
     ]
    }
   ],
   "source": [
    "# provide a global step counter that you can increment in the training loop if desired\n",
    "global_step = 0\n",
    "\n",
    "test_model(model, test_dl, device, global_step=global_step)\n",
    "for epoch in range(3):\n",
    "    for mel, labels in train_dl:\n",
    "        global_step += 1\n",
    "        mel, labels = mel.to(device), labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        logits = model(mel)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    print(f\"Epoch {epoch + 1}: train loss = {loss.item():.4f}\")\n",
    "    test_model(model, test_dl, device, global_step=global_step)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
