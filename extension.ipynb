{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2987387a",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "66d820d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import librosa\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e6e5ff",
   "metadata": {},
   "source": [
    "## Reproductibility : adding training seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bcb5da64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x176b31e1510>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 0\n",
    "rd.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883f5908",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "## Loading the genre label of each track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cf06518c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      track_id music_genre                         path  exists  label\n",
      "0            2     Hip-Hop  ..\\fma_small\\000\\000002.mp3    True      0\n",
      "1            5     Hip-Hop  ..\\fma_small\\000\\000005.mp3    True      0\n",
      "2           10         Pop  ..\\fma_small\\000\\000010.mp3    True      1\n",
      "3          140        Folk  ..\\fma_small\\000\\000140.mp3    True      2\n",
      "4          141        Folk  ..\\fma_small\\000\\000141.mp3    True      2\n",
      "...        ...         ...                          ...     ...    ...\n",
      "7995    154308     Hip-Hop  ..\\fma_small\\154\\154308.mp3    True      0\n",
      "7996    154309     Hip-Hop  ..\\fma_small\\154\\154309.mp3    True      0\n",
      "7997    154413         Pop  ..\\fma_small\\154\\154413.mp3    True      1\n",
      "7998    154414         Pop  ..\\fma_small\\154\\154414.mp3    True      1\n",
      "7999    155066     Hip-Hop  ..\\fma_small\\155\\155066.mp3    True      0\n",
      "\n",
      "[8000 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Loading the information about each track (metadata)\n",
    "path_of_fma_metadata = \"../fma_metadata/tracks.csv\"\n",
    "fma_metadata = pd.read_csv(path_of_fma_metadata, header=[0, 1], index_col=0)\n",
    "# Flatten columns multi-index\n",
    "fma_metadata.columns = ['__'.join(col).strip() for col in fma_metadata.columns.values]\n",
    "\n",
    "# Keeping only the genre column\n",
    "track_genre_df = fma_metadata.reset_index()[['track_id', 'track__genre_top']]\n",
    "track_genre_df = track_genre_df.rename(columns={\"track__genre_top\": \"music_genre\"})\n",
    "track_genre_df = track_genre_df.dropna(subset=[\"music_genre\"])\n",
    "\n",
    "\n",
    "# Function to get the audio file path from track_id\n",
    "def make_audio_path(track_id):\n",
    "    folder = f\"{track_id // 1000:03d}\"\n",
    "    filename = f\"{track_id:06d}.mp3\"\n",
    "    return Path(\"../fma_small\") / folder / filename\n",
    "\n",
    "track_genre_df[\"path\"] = track_genre_df[\"track_id\"].apply(make_audio_path)\n",
    "\n",
    "all_valid_files = {\n",
    "    p.resolve() for p in Path(\"../fma_small\").rglob(\"*.mp3\")\n",
    "}\n",
    "\n",
    "# Keep only rows where path exists\n",
    "track_genre_df[\"exists\"] = track_genre_df[\"path\"].apply(lambda p: p.resolve() in all_valid_files)\n",
    "track_genre_df = track_genre_df[track_genre_df[\"exists\"]].reset_index(drop=True)\n",
    "\n",
    "\n",
    "music_genres = track_genre_df[\"music_genre\"].unique().tolist()\n",
    "nbr_music_genres = len(music_genres)\n",
    "genre_to_idx = {g: i for i, g in enumerate(music_genres)}\n",
    "track_genre_df[\"label\"] = track_genre_df[\"music_genre\"].map(genre_to_idx)\n",
    "print(track_genre_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad9991b",
   "metadata": {},
   "source": [
    "## Preprocessing the data with Librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3221788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucie\\AppData\\Local\\Temp\\ipykernel_26040\\4120235938.py:13: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, _ = librosa.load(path, sr=None, mono=True)\n",
      "c:\\Users\\lucie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    }
   ],
   "source": [
    "#Precomputing mel-spectrogram and puting in cache\n",
    "CACHE_DIR = \"./mel_cache\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "MAX_LEN = 2600 # To unifirm the length of the mel-spectrograms\n",
    " \n",
    "def compute_and_cache_mel(track_id, path):\n",
    "    cache_path = f\"{CACHE_DIR}/{track_id}.npy\"\n",
    "\n",
    "    if os.path.isfile(cache_path):\n",
    "        return cache_path  # already cached\n",
    "\n",
    "    try:\n",
    "        audio, _ = librosa.load(path, sr=None, mono=True)\n",
    "        mel = librosa.feature.melspectrogram(y=audio, n_mels=128)\n",
    "        mel_db = librosa.power_to_db(mel)\n",
    "\n",
    "        # normalize\n",
    "        mel_db = (mel_db - mel_db.mean()) / (mel_db.std() + 1e-6)\n",
    "\n",
    "        T = mel_db.shape[1]\n",
    "        if T < MAX_LEN:\n",
    "            mel_db = np.pad(mel_db, ((0, 0), (0, MAX_LEN - T)), mode=\"constant\")\n",
    "        else:\n",
    "            mel_db = mel_db[:, :MAX_LEN]\n",
    "\n",
    "        \n",
    "        np.save(cache_path, mel_db.astype(np.float32))\n",
    "        return cache_path\n",
    "\n",
    "    except Exception:\n",
    "        return None \n",
    "\n",
    "mel_paths = []\n",
    "\n",
    "for i, row in track_genre_df.iterrows():\n",
    "    cache_path = compute_and_cache_mel(row[\"track_id\"], row[\"path\"])\n",
    "    mel_paths.append(cache_path)\n",
    "\n",
    "track_genre_df[\"mel_path\"] = mel_paths\n",
    "\n",
    "# keep only rows with valid cached mels\n",
    "track_genre_df = track_genre_df[track_genre_df[\"mel_path\"].notna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1937f0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicGenreDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, augment=False):\n",
    "        self.df = df\n",
    "        self.genre_to_idx = {g:i for i,g in enumerate(df[\"music_genre\"].unique())}\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        mel = np.load(row[\"mel_path\"])\n",
    "        if self.augment:\n",
    "            mel = self.augment_mel(mel)\n",
    "\n",
    "        mel = torch.tensor(mel, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        label = self.genre_to_idx[row[\"music_genre\"]]\n",
    "        return mel, label\n",
    "\n",
    "    def augment_mel(self, mel):\n",
    "        n_mels, T = mel.shape\n",
    "\n",
    "        # mask 10% of mel bins\n",
    "        f_pct = 0.1 \n",
    "        f = int(n_mels * f_pct)\n",
    "        f0 = np.random.randint(0, n_mels - f)\n",
    "        mel[f0:f0+f, :] = 0\n",
    "\n",
    "        # mask 10% of frames\n",
    "        t_pct = 0.1 \n",
    "        t = int(T * t_pct)\n",
    "        t0 = np.random.randint(0, T - t)\n",
    "        mel[:, t0:t0+t] = 0\n",
    "\n",
    "        return mel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390354ee",
   "metadata": {},
   "source": [
    "## Spliting the Dataset into Train/Test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9c1ab196",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = track_genre_df.index[\n",
    "    track_genre_df[\"path\"].apply(lambda p: os.path.isfile(p))\n",
    "].tolist()\n",
    "\n",
    "\n",
    "rd.shuffle(indices)\n",
    "\n",
    "split = int(0.8 * len(indices))\n",
    "train_indices = indices[:split]\n",
    "test_indices = indices[split:]\n",
    "\n",
    "train_dataset = Subset(MusicGenreDataset(track_genre_df), train_indices)\n",
    "test_dataset = Subset(MusicGenreDataset(track_genre_df), test_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423471db",
   "metadata": {},
   "source": [
    "# Baseline Implementation\n",
    "## Model : \n",
    "Small (2D) CNN with 3 convolutional layers (Conv → ReLU → MaxPool →\n",
    "Dropout), followed by a fully connected layer and softmax output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0b586f",
   "metadata": {},
   "source": [
    "Adding soft labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5da3ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_labels(labels, num_classes, smoothing=0.1):\n",
    "    with torch.no_grad():\n",
    "        smoothed = torch.full((labels.size(0), num_classes), smoothing / (num_classes - 1))\n",
    "        smoothed.scatter_(1, labels.unsqueeze(1), 1.0 - smoothing)\n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "593b129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our baseline model\n",
    "class CNN_base(nn.Module):\n",
    "    def __init__(self, num_classes=nbr_music_genres, dropout=0.25):\n",
    "        super().__init__()\n",
    "        channels = [1, 32, 64, 128, 256]\n",
    "        layers = []\n",
    "        for i in range(4):\n",
    "            layers += [\n",
    "                nn.Conv2d(channels[i], channels[i + 1], kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(channels[i+1]),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(2),\n",
    "                nn.Dropout2d(dropout)\n",
    "            ]\n",
    "           \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.gap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.gap(x)   \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47289d9",
   "metadata": {},
   "source": [
    " • Loss: Cross-entropy.\n",
    "\n",
    " • Optimizer: Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "94424233",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_base(num_classes=nbr_music_genres, dropout=0.25)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851f5f8",
   "metadata": {},
   "source": [
    " • Metric: Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8cd01df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_confusion_matrix(labels, preds, num_classes):\n",
    "    with torch.no_grad():\n",
    "        k = (labels >= 0) & (labels < num_classes)\n",
    "        inds = num_classes * labels[k] + preds[k]\n",
    "        cm = torch.bincount(inds, minlength=num_classes**2).reshape(num_classes, num_classes)\n",
    "    return cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8218197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dl, device, writer=None, global_step=None, class_names=None, name=\"\", smoothing=0.1):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for mel,label in test_dl:\n",
    "            mel = mel.to(device)\n",
    "            label = label.to(device)\n",
    "            preds = model(mel)\n",
    "            correct += (preds.argmax(1) == label).sum().item()\n",
    "            total += label.size(0)\n",
    "            num_classes = preds.shape[1]\n",
    "            smoothed = smooth_labels(\n",
    "                label, \n",
    "                num_classes=num_classes, \n",
    "                smoothing=smoothing\n",
    "            )\n",
    "\n",
    "            log_preds = torch.log_softmax(preds, dim=1)\n",
    "            loss = criterion(log_preds, smoothed)\n",
    "            losses.append(loss.item())\n",
    "            all_preds.append(preds.argmax(1).cpu())\n",
    "            all_labels.append(label.cpu())\n",
    "    if writer is not None and global_step is not None:\n",
    "        writer.add_scalar(f\"{name}/test/accuracy\", 100 * correct / total, global_step)\n",
    "        writer.add_scalar(f\"{name}/test/loss\", np.mean(losses), global_step)\n",
    "        all_preds = torch.cat(all_preds)\n",
    "        all_labels = torch.cat(all_labels)\n",
    "\n",
    "        num_classes = len(class_names) if class_names else 8\n",
    "        cm = torch.zeros((8, 8), dtype=torch.int64)\n",
    "        for t, p in zip(all_labels, all_preds):\n",
    "            cm[t, p] += 1\n",
    "\n",
    "        # TensorBoard display\n",
    "        \n",
    "        fig = plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=False, cmap=\"Blues\", \n",
    "                    xticklabels=class_names if class_names else range(num_classes),\n",
    "                    yticklabels=class_names if class_names else range(num_classes))\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"True\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "\n",
    "        writer.add_figure(\"test/confusion_matrix\", fig, global_step)\n",
    "        plt.close(fig)\n",
    "\n",
    "    print(f\"Test accuracy: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3fd5e2",
   "metadata": {},
   "source": [
    "# Run the training loop\n",
    "\n",
    "Use \n",
    "[tensorboard]\n",
    "( https://docs.pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) \n",
    "to monitor training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "382a2123",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dl = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "# log a small batch of images and the model graph (if possible)\n",
    "imgs_sample, labels_sample = next(iter(train_dl))\n",
    "imgs_sample = imgs_sample.to(device)\n",
    "grid = make_grid(imgs_sample[:16], nrow=4, normalize=True, scale_each=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "313417cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 12.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/200 [13:00<?, ?it/s]\n",
      "Epoch 1:   0%|          | 0/200 [36:40<?, ?it/s, loss=1.51]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 20.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/200 [39:25<?, ?it/s, loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 27.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/200 [26:51<?, ?it/s, loss=1.49]\n",
      "Epoch 3:   0%|          | 0/200 [30:30<?, ?it/s, loss=1.43]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 27.64%\n"
     ]
    }
   ],
   "source": [
    "# provide a global step counter that you can increment in the training loop if desired\n",
    "global_step = 0\n",
    "writer = SummaryWriter(log_dir=\"./runs/genre_classif\")\n",
    "\n",
    "test_model(model, test_dl, device, global_step=global_step)\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    train_bar = tqdm(train_dl, desc=f\"Epoch {epoch+1}\")\n",
    "    for mel, labels in train_dl:\n",
    "        global_step += 1\n",
    "        mel, labels = mel.to(device), labels.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        logits = model(mel)\n",
    "        num_classes = logits.shape[1]\n",
    "        smoothed = smooth_labels(labels, num_classes=num_classes, smoothing=0.1)\n",
    "        log_preds = torch.log_softmax(logits, dim=1)\n",
    "        loss = criterion(log_preds, smoothed)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        writer.add_scalar(\"train/loss\", loss.item(), global_step)\n",
    "        train_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    test_model(model, test_dl, device, writer, global_step, class_names=music_genres)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
